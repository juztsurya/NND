import torch
import torch.nn.functional as F
import matplotlib.pyplot as plt
def scaled_dot_product_attention(Q, K, V, mask=None):
    d_k = Q.size(-1)
    scores = torch.matmul(Q, K.transpose(-2, -1)) / torch.sqrt(torch.tensor(d_k))
    if mask is not None:
        scores.masked_fill_(mask == 0, -1e9)
    attn_weights = F.softmax(scores, dim=-1)
    return torch.matmul(attn_weights, V), attn_weights
# Q,K,V from input + visualize
X = torch.rand(1, 6, 64)
Q = torch.nn.Linear(64, 64)(X)
K = torch.nn.Linear(64, 64)(X) 
V = torch.nn.Linear(64, 64)(X)
output, weights = scaled_dot_product_attention(Q, K, V)
print(output.shape)
plt.imshow(weights[0].detach(), cmap='Blues')
plt.title('Attention Weights')
plt.show()
